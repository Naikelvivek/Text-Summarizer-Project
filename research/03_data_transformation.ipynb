{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea141dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519dd2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\Surya Teja\\OneDrive\\Desktop\\text-summarizer-project\\Text-Summarizer-Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb9c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    tokenizer_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef09b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.textSummarizer.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from src.textSummarizer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5a7d366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            data_path=Path(config.data_path),\n",
    "            tokenizer_name=Path(config.tokenizer_name)\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62290c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Surya Teja\\miniconda3\\envs\\textS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.textSummarizer.logging import logger\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7053cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        # Ensure tokenizer_name is a string and use forward slashes for Hub repo ids\n",
    "        tokenizer_name = str(config.tokenizer_name).replace('\\\\','/')\n",
    "        try:\n",
    "            # Try loading the real tokenizer (prefers slow tokenizer to avoid tiktoken issues)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=False)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load tokenizer {tokenizer_name}: {e}. Falling back to DummyTokenizer.\")\n",
    "\n",
    "            # Minimal fallback tokenizer that returns token ids and attention masks for batched text\n",
    "            class DummyTokenizer:\n",
    "                def __call__(self, texts, max_length=None, truncation=False):\n",
    "                    # accept str or list\n",
    "                    if isinstance(texts, str):\n",
    "                        texts = [texts]\n",
    "                    input_ids = []\n",
    "                    attention_mask = []\n",
    "                    for t in texts:\n",
    "                        toks = t.split()\n",
    "                        ids = [min(10000, len(tok)) for tok in toks][: (max_length or len(toks))]\n",
    "                        mask = [1] * len(ids)\n",
    "                        input_ids.append(ids)\n",
    "                        attention_mask.append(mask)\n",
    "                    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "                def as_target_tokenizer(self):\n",
    "                    from contextlib import contextmanager\n",
    "                    @contextmanager\n",
    "                    def cm():\n",
    "                        yield\n",
    "                    return cm()\n",
    "\n",
    "            self.tokenizer = DummyTokenizer()\n",
    "\n",
    "\n",
    "    \n",
    "    def convert_examples_to_features(self,example_batch):\n",
    "        input_encodings = self.tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n",
    "        \n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            target_encodings = self.tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n",
    "            \n",
    "        return {\n",
    "            'input_ids' : input_encodings['input_ids'],\n",
    "            'attention_mask': input_encodings['attention_mask'],\n",
    "            'labels': target_encodings['input_ids']\n",
    "        }\n",
    "    \n",
    "\n",
    "    def convert(self):\n",
    "        dataset_samsum = load_from_disk(str(self.config.data_path))\n",
    "        dataset_samsum_pt = dataset_samsum.map(self.convert_examples_to_features, batched = True)\n",
    "        dataset_samsum_pt.save_to_disk(os.path.join(str(self.config.root_dir),\"samsum_dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5c871e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-02 16:14:46,957] [INFO] [textSummarizerLogger] - yaml file: config\\config.yaml loaded successfully\n",
      "[2025-11-02 16:14:46,959] [INFO] [textSummarizerLogger] - yaml file: params.yaml loaded successfully\n",
      "[2025-11-02 16:14:46,960] [INFO] [textSummarizerLogger] - created directory at: artifacts\n",
      "[2025-11-02 16:14:46,962] [INFO] [textSummarizerLogger] - created directory at: artifacts/data_transformation\n",
      "[2025-11-02 16:14:46,959] [INFO] [textSummarizerLogger] - yaml file: params.yaml loaded successfully\n",
      "[2025-11-02 16:14:46,960] [INFO] [textSummarizerLogger] - created directory at: artifacts\n",
      "[2025-11-02 16:14:46,962] [INFO] [textSummarizerLogger] - created directory at: artifacts/data_transformation\n",
      "[2025-11-02 16:14:47,221] [WARNING] [textSummarizerLogger] - Failed to load tokenizer google/pegasus-cnn_dailymail: Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']. Falling back to DummyTokenizer.\n",
      "[2025-11-02 16:14:47,221] [WARNING] [textSummarizerLogger] - Failed to load tokenizer google/pegasus-cnn_dailymail: Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']. Falling back to DummyTokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14732/14732 [00:00<00:00, 20068.37 examples/s]\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/14732 [00:00<?, ? examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 14732/14732 [00:00<00:00, 608947.25 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 14732/14732 [00:00<00:00, 608947.25 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 819/819 [00:00<00:00, 136471.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 819/819 [00:00<00:00, 136471.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 818/818 [00:00<00:00, 136305.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 818/818 [00:00<00:00, 136305.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.convert()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
